{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from os import path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import path, listdir\n",
    "import matplotlib.pyplot as plt\n",
    "import gaze_heatmap as gh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'breakout'\n",
    "trial_name = '519_AS_4083721_Jul-25-14-44-15'\n",
    "trial_name_elements = trial_name.split('_')\n",
    "img_prefix = trial_name_elements[1]+'_'+trial_name_elements[2]+'_'\n",
    "\n",
    "# TODO: randomly sample 10 images from trial\n",
    "start_img_idx = 5000\n",
    "data_dir = '/home/akanksha/learning-rewards-of-learners/data/novice-atari-head/'\n",
    "traj_dir = path.join(path.join(data_dir, env_name),trial_name)\n",
    "\n",
    "#load data\n",
    "if env_name in ['mspacman', 'centipede', 'asterix', 'berzerk', 'hero', 'enduro']:\n",
    "    crop_top = False  # mask out few rows from the bottom\n",
    "else:\n",
    "    crop_top = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaxSkipAndWarpFrames(traj_dir, start_img_idx):\n",
    "    \"\"\"take a trajectory file of frames and max over every 3rd and 4th observation\"\"\"\n",
    "    skip = 4  \n",
    "    sample_pic = np.random.choice(\n",
    "        listdir(path.join(traj_dir)))\n",
    "    image_path = path.join(traj_dir, sample_pic)\n",
    "    print(image_path)\n",
    "    pic = cv2.imread(image_path)\n",
    "    obs_buffer = np.zeros((2,)+pic.shape, dtype=np.uint8)\n",
    "    print(obs_buffer.shape)\n",
    "    max_frames = []\n",
    "    for i in range(16):\n",
    "        img_name = traj_dir+'/'+img_prefix+str(start_img_idx+i)+'.png' \n",
    "#         print(img_name)\n",
    "\n",
    "        if i % skip == skip - 2:\n",
    "            obs = cv2.imread(img_name)\n",
    "            print(img_name)\n",
    "#             print(obs)\n",
    "            \n",
    "            obs_buffer[0] = obs\n",
    "        if i % skip == skip - 1:\n",
    "            obs = cv2.imread(img_name)\n",
    "            obs_buffer[1] = obs\n",
    "\n",
    "            # warp max to 80x80 grayscale\n",
    "            image = obs_buffer.max(axis=0)\n",
    "            warped = utils.GrayScaleWarpImage(image)\n",
    "            max_frames.append(warped)\n",
    "#     print('frames: ', len(max_frames))\n",
    "    return max_frames\n",
    "\n",
    "\n",
    "def StackFrames(frames):\n",
    "    import copy\n",
    "    \"\"\"stack every four frames to make an observation (84,84,4)\"\"\"\n",
    "    stacked = []\n",
    "    stacked_obs = np.zeros((84, 84, 4))\n",
    "    for i in range(len(frames)):\n",
    "        if i >= 3:\n",
    "            stacked_obs[:, :, 0] = frames[i-3]\n",
    "            stacked_obs[:, :, 1] = frames[i-2]\n",
    "            stacked_obs[:, :, 2] = frames[i-1]\n",
    "            stacked_obs[:, :, 3] = frames[i]\n",
    "            stacked.append(np.expand_dims(copy.deepcopy(stacked_obs), 0))\n",
    "    print('stacked frames: ', len(stacked))\n",
    "    return stacked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 /home/akanksha/learning-rewards-of-learners/data/novice-atari-head/breakout/519_AS_4083721_Jul-25-14-44-15\n",
      "/home/akanksha/learning-rewards-of-learners/data/novice-atari-head/breakout/519_AS_4083721_Jul-25-14-44-15/AS_4083721_7187.png\n",
      "(2, 210, 160, 3)\n",
      "/home/akanksha/learning-rewards-of-learners/data/novice-atari-head/breakout/519_AS_4083721_Jul-25-14-44-15/AS_4083721_5002.png\n",
      "/home/akanksha/learning-rewards-of-learners/data/novice-atari-head/breakout/519_AS_4083721_Jul-25-14-44-15/AS_4083721_5006.png\n",
      "/home/akanksha/learning-rewards-of-learners/data/novice-atari-head/breakout/519_AS_4083721_Jul-25-14-44-15/AS_4083721_5010.png\n",
      "/home/akanksha/learning-rewards-of-learners/data/novice-atari-head/breakout/519_AS_4083721_Jul-25-14-44-15/AS_4083721_5014.png\n",
      "stacked frames:  1\n"
     ]
    }
   ],
   "source": [
    "### image stack --> GT gaze\n",
    "\n",
    "#load 16 consecutive images starting from start_img_idx\n",
    "imgs = []\n",
    "for i in range(16):\n",
    "    img = cv2.imread(traj_dir+'/'+img_prefix+str(start_img_idx+i)+'.png')\n",
    "    imgs.append(img)\n",
    "\n",
    "# Skip, Mask, Stack Images\n",
    "print(start_img_idx, traj_dir)\n",
    "maxed_traj = MaxSkipAndWarpFrames(traj_dir, start_img_idx)\n",
    "stacked_traj = StackFrames(maxed_traj)\n",
    "masked = utils.mask_score(utils.normalize_state(stacked_traj[0]), env_name, crop_top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15,15))\n",
    "c=0\n",
    "# first line is the metadata, second is the header\n",
    "# indices of frames in the stack \n",
    "k = range(start_img_idx, start_img_idx+16, 4) # every 4th frame\n",
    "k1 = range(start_img_idx, start_img_idx+16) # 16 consecutive frames\n",
    "gaze = []\n",
    "conv_size = 20\n",
    "height, width = imgs[0].shape[0], imgs[0].shape[1]\n",
    "gt_gaze = np.zeros((height,width,3))\n",
    "last_img = imgs[15]\n",
    "\n",
    "## load corresponding GT gaze using asc and text file\n",
    "# raw gaze coord circles on image\n",
    "with open(traj_dir+'.txt') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i in k1: #coz 1st line in text file is metadata\n",
    "            #frame,reward,score,terminal, action\n",
    "            curr_data = line.rstrip('\\n').split(',')\n",
    "            gaze_coords = [float(gp) if gp != 'null' else float('nan') for gp in curr_data[6:]]\n",
    "            gaze.append(gaze_coords)\n",
    "            \n",
    "            if i in k:\n",
    "                c+=1\n",
    "                # overlay gaze coordinates on img\n",
    "                img_full = imgs[c*4-1]\n",
    "                #  normalized image to scaled and converted to uint8 for image processing\n",
    "                img = 255*masked[:,:,:,c-1].squeeze()\n",
    "                img = img.astype(np.uint8)\n",
    "\n",
    "                h, w = img.shape[0], img.shape[1]\n",
    "\n",
    "                for j in range(0,len(gaze_coords),2):\n",
    "    #                 if('null' not in gaze_coords[j]):\n",
    "                    x = float(gaze_coords[j])\n",
    "                    y = float(gaze_coords[j+1])\n",
    "                    x_norm, y_norm = x/width, y/height\n",
    "                    x1, y1 = int(x_norm*w), int(y_norm*h)\n",
    "                    cv2.circle(img, (x1,y1), 3, (255,255,255), thickness=1, lineType=8)\n",
    "                    cv2.circle(img_full, (int(x),int(y)), 3, (255,255,255), thickness=1, lineType=8)\n",
    "                    cv2.circle(gt_gaze, (int(x),int(y)), 3, (255,255,255), thickness=1, lineType=8)\n",
    "                    cv2.circle(last_img, (int(x),int(y)), 3, (255,255,255), thickness=1, lineType=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 84, 84, 4)\n"
     ]
    }
   ],
   "source": [
    "# image stack, trained BCO model \n",
    "import cnn\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.device('cuda:0')\n",
    "\n",
    "#TODO: get model name from params\n",
    "model_path = '/home/akanksha/Documents/behavioral_cloning_atari/checkpoints_dropout_novices/breakout/breakout_dropout_network.pth.tar'\n",
    "model = cnn.Network(num_output_actions=4)\n",
    "chkpt = torch.load(model_path, map_location='cpu')\n",
    "model.load_state_dict(chkpt['state_dict'])\n",
    "\n",
    "# # plot heatmap\n",
    "mask_size = 3\n",
    "print(masked.shape)\n",
    "# img_stack = masked.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 20, 20]) torch.Size([1, 64, 9, 9]) torch.Size([1, 64, 7, 7])\n",
      "gaze_coords length:  16\n",
      "Warning: did you provide the correct gaze data? Because the data for only 16 frames is detected\n",
      "WARNING: Gaussian filter's sigma is 0, i.e. no blur.\n",
      "16 <class 'numpy.ndarray'>\n",
      "(20, 20, 1)\n",
      "gaze frames:  4\n",
      "stacked gaze frames:  1\n",
      "gaze_coords length:  16\n",
      "Warning: did you provide the correct gaze data? Because the data for only 16 frames is detected\n",
      "WARNING: Gaussian filter's sigma is 0, i.e. no blur.\n",
      "16 <class 'numpy.ndarray'>\n",
      "(9, 9, 1)\n",
      "gaze frames:  4\n",
      "stacked gaze frames:  1\n",
      "gaze_coords length:  16\n",
      "Warning: did you provide the correct gaze data? Because the data for only 16 frames is detected\n",
      "WARNING: Gaussian filter's sigma is 0, i.e. no blur.\n",
      "16 <class 'numpy.ndarray'>\n",
      "(7, 7, 1)\n",
      "gaze frames:  4\n",
      "stacked gaze frames:  1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# image stack, trained BCO model --> n conv_layer activations \n",
    "# some code already exists in trex repo\n",
    "import torch\n",
    "device = torch.device(\"cpu\")\n",
    "input_stack = torch.from_numpy(masked)\n",
    "input_stack = input_stack.view(-1,4,84,84)\n",
    "conv1,conv2,conv3,_,out_actions = model(input_stack.float().to(device))\n",
    "print(conv1.shape, conv2.shape, conv3.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "h = gh.DatasetWithHeatmap()\n",
    "\n",
    "#20x20\n",
    "conv_size = 20\n",
    "g = h.createGazeHeatmap(gaze, conv_size)\n",
    "maxed_gaze = utils.MaxSkipGaze(g, conv_size)               \n",
    "stacked_gaze_20 = utils.CollapseGaze(maxed_gaze, conv_size)\n",
    "ax3 = fig.add_subplot(3,4,1)\n",
    "ax3.imshow(stacked_gaze_20[0].squeeze()*255, cmap='seismic')\n",
    "ax3.set_axis_off()\n",
    "# ax3.title.set_text('Gaze from txt')\n",
    "\n",
    "# normalize and collapse conv1 map\n",
    "conv_gaze = conv1.sum(dim=1)\n",
    "batch_size = conv1.shape[0]\n",
    "min_x = torch.min(torch.min(conv_gaze, dim=1)[0], dim=1)[0]\n",
    "max_x = torch.max(torch.max(conv_gaze, dim=1)[0], dim=1)[0]\n",
    "\n",
    "min_x = min_x.reshape(batch_size, 1).repeat(\n",
    "    1, conv_size).unsqueeze(-1).expand(batch_size, conv_size, conv_size)\n",
    "max_x = max_x.reshape(batch_size, 1).repeat(\n",
    "    1, conv_size).unsqueeze(-1).expand(batch_size, conv_size, conv_size)\n",
    "x_norm_20 = (conv_gaze - min_x)/(max_x - min_x)\n",
    "x_norm_20 = x_norm_20.data.numpy()\n",
    "ax3 = fig.add_subplot(3,4,2)\n",
    "ax3.imshow(x_norm_20.squeeze()*255, cmap='seismic')\n",
    "ax3.set_axis_off()\n",
    "\n",
    "#9x9\n",
    "conv_size = 9\n",
    "g = h.createGazeHeatmap(gaze, conv_size)\n",
    "maxed_gaze = utils.MaxSkipGaze(g, conv_size)               \n",
    "stacked_gaze_9 = utils.CollapseGaze(maxed_gaze, conv_size)\n",
    "ax3 = fig.add_subplot(3,4,5)\n",
    "ax3.imshow(stacked_gaze_9[0].squeeze()*255, cmap='seismic')\n",
    "ax3.set_axis_off()\n",
    "\n",
    "# normalize and collapse conv2 map\n",
    "conv_gaze = conv2.sum(dim=1)\n",
    "batch_size = conv2.shape[0]\n",
    "min_x = torch.min(torch.min(conv_gaze, dim=1)[0], dim=1)[0]\n",
    "max_x = torch.max(torch.max(conv_gaze, dim=1)[0], dim=1)[0]\n",
    "\n",
    "min_x = min_x.reshape(batch_size, 1).repeat(\n",
    "    1, conv_size).unsqueeze(-1).expand(batch_size, conv_size, conv_size)\n",
    "max_x = max_x.reshape(batch_size, 1).repeat(\n",
    "    1, conv_size).unsqueeze(-1).expand(batch_size, conv_size, conv_size)\n",
    "x_norm_9 = (conv_gaze - min_x)/(max_x - min_x)\n",
    "x_norm_9 = x_norm_9.data.numpy()\n",
    "ax3 = fig.add_subplot(3,4,6)\n",
    "ax3.imshow(x_norm_9.squeeze()*255, cmap='seismic')\n",
    "ax3.set_axis_off()\n",
    "\n",
    "#7x7\n",
    "conv_size = 7\n",
    "g = h.createGazeHeatmap(gaze, conv_size)\n",
    "maxed_gaze = utils.MaxSkipGaze(g, conv_size)               \n",
    "stacked_gaze_7 = utils.CollapseGaze(maxed_gaze, conv_size)\n",
    "ax3 = fig.add_subplot(3,4,9)\n",
    "ax3.imshow(stacked_gaze_7[0].squeeze()*255, cmap='seismic')\n",
    "ax3.set_axis_off()\n",
    "\n",
    "# normalize and collapse conv3 map\n",
    "conv_gaze = conv3.sum(dim=1)\n",
    "batch_size = conv3.shape[0]\n",
    "min_x = torch.min(torch.min(conv_gaze, dim=1)[0], dim=1)[0]\n",
    "max_x = torch.max(torch.max(conv_gaze, dim=1)[0], dim=1)[0]\n",
    "\n",
    "min_x = min_x.reshape(batch_size, 1).repeat(\n",
    "    1, conv_size).unsqueeze(-1).expand(batch_size, conv_size, conv_size)\n",
    "max_x = max_x.reshape(batch_size, 1).repeat(\n",
    "    1, conv_size).unsqueeze(-1).expand(batch_size, conv_size, conv_size)\n",
    "x_norm_7 = (conv_gaze - min_x)/(max_x - min_x)\n",
    "x_norm_7 = x_norm_7.data.numpy()\n",
    "ax3 = fig.add_subplot(3,4,10)\n",
    "ax3.imshow(x_norm_7.squeeze()*255, cmap='seismic')\n",
    "ax3.set_axis_off()\n",
    "\n",
    "plt.savefig('breakout_dropout_collapsed_conv.png',bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20) (20, 20)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1b2ee17bdb1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mconv_20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_norm_20\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgaze_20\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_20\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgaze_20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconv_20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# conv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenv/t-rex/lib/python3.6/site-packages/scipy/stats/stats.py\u001b[0m in \u001b[0;36mpearsonr\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   3040\u001b[0m     \u001b[0;31m# Presumably, if abs(r) > 1, then it is only some small artifact of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3041\u001b[0m     \u001b[0;31m# floating point arithmetic.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3042\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3043\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3044\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# compute correlations\n",
    "# 2D GT gaze map and 2D collapsed conv maps\n",
    "import scipy\n",
    "\n",
    "# conv 1\n",
    "gaze_20 = stacked_gaze_20[0].squeeze()\n",
    "conv_20 = x_norm_20.squeeze()\n",
    "print(gaze_20.shape, conv_20.shape)\n",
    "scipy.stats.pearsonr(gaze_20,conv_20)\n",
    "\n",
    "# conv2\n",
    "\n",
    "# conv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gazeNetwork",
   "language": "python",
   "name": "gazenetwork"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
